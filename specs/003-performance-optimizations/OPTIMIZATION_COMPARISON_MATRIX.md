# GPU Optimization Techniques - Comparison Matrix

**Date**: 2025-12-26
**Context**: Real-time audio synthesis with PyTorch (Metal/CUDA)
**Target**: <100ms synthesis latency, 10+ concurrent users, 30% resource reduction

---

## Quick Decision Matrix

| Technique | Impact | Effort | Risk | Metal | CUDA | Priority | Phase |
|-----------|--------|--------|------|-------|------|----------|-------|
| **Memory Pre-Allocation** | üî¥ High | üü¢ Low | üü¢ Low | ‚úÖ | ‚úÖ | P0 | 1 |
| **Batch Voice Rendering** | üî¥ High | üü° Med | üü¢ Low | ‚úÖ | ‚úÖ | P0 | 1 |
| **torch.no_grad() Context** | üü° Med | üü¢ Low | üü¢ Low | ‚úÖ | ‚úÖ | P0 | 1 |
| **torch.compile (JIT)** | üü° Med | üü° Med | üü° Med | ‚ö†Ô∏è | ‚úÖ | P1 | 2 |
| **Kernel Fusion (Manual)** | üü° Med | üü° Med | üü¢ Low | ‚úÖ | ‚úÖ | P1 | 2 |
| **CUDA Streams (Async)** | üü° Med | üî¥ High | üü° Med | ‚ùå | ‚úÖ | P2 | 3 |
| **Profiling Integration** | üü° Med | üü¢ Low | üü¢ Low | ‚úÖ | ‚úÖ | P1 | 1 |
| **Mixed Precision (FP16)** | üü¢ Low | üü¢ Low | üî¥ **Critical** | ‚ùå | ‚ö†Ô∏è | **AVOID** | N/A |
| **CUDA Graphs** | üî¥ High | üî¥ High | üî¥ High | ‚ùå | ‚úÖ | P3 | Future |
| **Custom CUDA Kernels** | üî¥ High | üî¥ Very High | üî¥ Very High | ‚ùå | ‚úÖ | P4 | Research |

**Legend**:
- Impact: üî¥ High (>30% improvement) | üü° Medium (10-30%) | üü¢ Low (<10%)
- Effort: üü¢ Low (1-2 days) | üü° Medium (3-5 days) | üî¥ High (1+ weeks)
- Risk: üü¢ Low (safe, reversible) | üü° Medium (requires testing) | üî¥ High (quality/stability concerns)
- Platform: ‚úÖ Supported | ‚ö†Ô∏è Limited | ‚ùå Not Available
- Priority: P0 (Critical) ‚Üí P4 (Future Research)

---

## Detailed Comparison

### 1. Memory Pre-Allocation

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | N/A (reduces variance, not average) |
| **Primary Benefit** | Eliminates GC pauses (audio glitch prevention) |
| **Implementation** | Pre-allocate buffers in `__init__`, reuse via slicing |
| **Memory Overhead** | +5-10 MB baseline (negligible) |
| **Metal Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (unified memory benefits) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (prevents fragmentation) |
| **Code Complexity** | +5% (simple, one-time change) |
| **Regression Risk** | Very Low (deterministic buffers) |
| **Recommendation** | **IMPLEMENT FIRST** - highest reliability impact |

**Code Snippet**:
```python
# In SynthesisEngine.__init__()
self.max_phrase_samples = int(30.0 * self.sample_rate)
self.audio_buffer_pool = torch.zeros(
    self.max_phrase_samples,
    device=self.device,
    dtype=torch.float32
)
```

---

### 2. Batch Voice Rendering

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 40-60% for chord synthesis |
| **Primary Benefit** | Maximize GPU parallelism (SIMD operations) |
| **Implementation** | Vectorize multi-voice synthesis (chords, overlapping notes) |
| **Memory Overhead** | Temporary: batch_size √ó max_duration samples |
| **Metal Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê (excellent for small batches <16) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (scales to large batches 32+) |
| **Code Complexity** | +20% (requires refactoring render loops) |
| **Regression Risk** | Low (isolated to chord rendering initially) |
| **Recommendation** | **IMPLEMENT EARLY** - highest performance gain |

**Code Snippet**:
```python
# Batch all chord voices
pitches = torch.tensor([root-12] + [root+i for i in intervals], device=self.device)
freqs = 440.0 * (2.0 ** ((pitches - 69.0) / 12.0))
# Vectorized oscillator generation...
```

---

### 3. torch.no_grad() Context

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 5-10% (prevents gradient overhead) |
| **Primary Benefit** | Memory leak prevention, reduce computation graph building |
| **Implementation** | Wrap all inference calls with `with torch.no_grad():` |
| **Memory Overhead** | -10-15% (reduction from disabling gradients) |
| **Metal Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê (consistent benefit) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê (consistent benefit) |
| **Code Complexity** | +1% (one-line context wrapper) |
| **Regression Risk** | None (inference-only code) |
| **Recommendation** | **IMPLEMENT IMMEDIATELY** - trivial, zero risk |

**Code Snippet**:
```python
def render_phrase(self, ...):
    with torch.no_grad():  # Single line addition
        # All synthesis operations...
```

---

### 4. torch.compile (JIT Compilation)

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 20-40% (CUDA), 10-20% (Metal) |
| **Primary Benefit** | Automatic kernel fusion, optimized dispatch |
| **Implementation** | `torch.compile(model.forward, mode="reduce-overhead")` |
| **Memory Overhead** | +10-20 MB (compiled kernel cache) |
| **Metal Performance** | ‚≠ê‚≠ê‚≠ê (newer backend, less optimized) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (mature TorchInductor backend) |
| **Code Complexity** | +5% (warmup logic required) |
| **Regression Risk** | Medium (JIT compilation latency on first call) |
| **Recommendation** | Phase 2 - test warmup overhead carefully |

**Critical Caveat**:
```python
# MUST warmup during init (avoid first-call JIT penalty)
def __init__(self, ...):
    self.forward = torch.compile(self.forward, mode="reduce-overhead")
    # Warmup
    dummy_pitch = torch.tensor(60.0, device=self.device)
    _ = self(dummy_pitch, duration_samples=1000, velocity=0.5)
```

---

### 5. Manual Kernel Fusion

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 10-20% (reduces kernel launch overhead) |
| **Primary Benefit** | Combine sequential ops into single expression |
| **Implementation** | Merge separate tensor operations into compound expressions |
| **Memory Overhead** | Neutral (eliminates intermediate tensors) |
| **Metal Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê (benefits from reduced kernel launches) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê (benefits from reduced kernel launches) |
| **Code Complexity** | +10% (less readable expressions) |
| **Regression Risk** | Low (deterministic transformations) |
| **Recommendation** | Phase 2 - apply to hot paths after profiling |

**Example**:
```python
# Before (3 kernels)
osc = torch.sin(2.0 * math.pi * freq * t)
env = torch.exp(-5.0 * t / 0.15)
output = osc * env * velocity

# After (1 kernel)
output = torch.sin(2.0 * math.pi * freq * t) * torch.exp(-5.0 * t / 0.15) * velocity
```

---

### 6. CUDA Streams (Asynchronous Operations)

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 15-25% (total delivery time, not synthesis alone) |
| **Primary Benefit** | Overlap GPU compute with CPU-GPU transfers |
| **Implementation** | Use `torch.cuda.Stream()` for concurrent operations |
| **Memory Overhead** | Minimal (stream context) |
| **Metal Performance** | N/A (automatic pipelining by MPS backend) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê (explicit stream control) |
| **Code Complexity** | +30% (async logic, synchronization) |
| **Regression Risk** | Medium (concurrency bugs, race conditions) |
| **Recommendation** | Phase 3 - CUDA-specific optimization |

**Platform Note**: Metal handles pipelining automatically; no manual stream management needed.

---

### 7. Profiling Integration

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | N/A (measurement, not optimization) |
| **Primary Benefit** | Identify actual bottlenecks, prevent regressions |
| **Implementation** | `torch.profiler` + CI benchmarks |
| **Memory Overhead** | Negligible (profiling overhead ~5% during capture) |
| **Metal Performance** | ‚úÖ Metal System Trace (Instruments) |
| **CUDA Performance** | ‚úÖ NSight Systems, nvidia-smi |
| **Code Complexity** | +10% (profiling infrastructure) |
| **Regression Risk** | None (observability only) |
| **Recommendation** | **IMPLEMENT IN PHASE 1** - foundation for all optimizations |

**Essential Tools**:
- `torch.profiler` ‚Üí Chrome tracing visualization
- CUDA: `nvidia-smi`, NSight Systems
- Metal: Xcode Instruments (Metal System Trace)

---

### 8. Mixed Precision (FP16/BF16)

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 10-15% (theoretical, for neural networks) |
| **Primary Benefit** | ‚ùå **NONE for audio synthesis** |
| **Implementation** | **DO NOT IMPLEMENT** |
| **Memory Overhead** | -50% (reduced precision) |
| **Metal Performance** | ‚ùå Limited FP16 support, no BF16 |
| **CUDA Performance** | ‚ö†Ô∏è Available but **DEGRADES AUDIO QUALITY** |
| **Code Complexity** | +15% (precision management) |
| **Regression Risk** | üî¥ **CRITICAL** - audible quantization noise |
| **Recommendation** | **AVOID ENTIRELY** - unacceptable quality trade-off |

**Why Rejected**:
- Audio requires 5-6 digits precision; FP16 provides ~3 digits
- Voice accumulation in FP16 compounds rounding errors ‚Üí clicks, pops
- Minimal performance gain (10-15%) not worth quality degradation
- **Verdict**: Use FP32 for ALL audio operations

---

### 9. CUDA Graphs

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 30-50% (for repetitive, static patterns) |
| **Primary Benefit** | Pre-capture execution graph, eliminate overhead |
| **Implementation** | `torch.cuda.CUDAGraph()` with graph replay |
| **Memory Overhead** | +20-50 MB (captured graph state) |
| **Metal Performance** | ‚ùå Not available |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (when applicable) |
| **Code Complexity** | +40% (graph capture, replay logic) |
| **Regression Risk** | High (requires static input shapes, limited flexibility) |
| **Recommendation** | Phase 3 - research if phrase patterns are repetitive |

**Constraint**: Requires identical tensor shapes across invocations. Ambient music's variability may limit applicability.

---

### 10. Custom CUDA Kernels

| Aspect | Details |
|--------|---------|
| **Latency Reduction** | 50-100% (for specific operations) |
| **Primary Benefit** | Hand-optimized assembly for critical paths |
| **Implementation** | C++/CUDA kernel development, PyTorch extension |
| **Memory Overhead** | Neutral (replaces PyTorch ops) |
| **Metal Performance** | ‚ùå Not applicable (Metal Shading Language alternative) |
| **CUDA Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (maximum possible performance) |
| **Code Complexity** | +200% (separate C++ codebase, build complexity) |
| **Regression Risk** | Very High (numerical precision, edge cases) |
| **Recommendation** | Phase 4+ - only if PyTorch ops insufficient |

**Effort Estimate**: 1-2 weeks per kernel, requires CUDA expertise.

---

## Platform-Specific Recommendations

### Apple Silicon (Metal/MPS)

**Implement**:
1. ‚úÖ Memory pre-allocation (highest impact for unified memory)
2. ‚úÖ Batch voice rendering (good performance, <16 voices)
3. ‚úÖ torch.no_grad() context
4. ‚ö†Ô∏è torch.compile (test carefully, newer backend)
5. ‚úÖ Manual kernel fusion
6. ‚úÖ Metal System Trace profiling

**Avoid**:
- ‚ùå CUDA streams (not applicable, automatic pipelining)
- ‚ùå Mixed precision (limited support, quality concerns)
- ‚ùå CUDA graphs/kernels (CUDA-only)

**Expected Baseline**: ~50-80ms synthesis latency after Phase 1-2 optimizations.

---

### NVIDIA GPU (CUDA)

**Implement**:
1. ‚úÖ Memory pre-allocation (prevent fragmentation)
2. ‚úÖ Batch voice rendering (excellent scaling >32 voices)
3. ‚úÖ torch.no_grad() context
4. ‚úÖ torch.compile (mature backend, high gains)
5. ‚úÖ Manual kernel fusion
6. ‚úÖ CUDA streams (async overlap)
7. ‚úÖ NSight profiling

**Advanced (Phase 3+)**:
- ‚ö†Ô∏è CUDA graphs (if phrase patterns repeat)
- ‚ö†Ô∏è Custom kernels (last resort optimization)

**Avoid**:
- ‚ùå Mixed precision for audio (quality degradation)

**Expected Baseline**: ~30-50ms synthesis latency after Phase 1-2 optimizations.

---

## Implementation Sequence

### Phase 1: Foundation (Week 1) - Priority: P0

| Day | Task | Expected Outcome |
|-----|------|------------------|
| 1-2 | Memory pre-allocation | Eliminate GC glitches, stable latency |
| 2 | torch.no_grad() wrapper | -10% memory usage |
| 3 | Profiling baseline | Documented bottlenecks, regression thresholds |
| 4-5 | Batch chord rendering | -40% chord synthesis time |

**Cumulative Gain**: 30-40% overall latency reduction, zero audio glitches.

---

### Phase 2: Performance (Week 2-3) - Priority: P1

| Day | Task | Expected Outcome |
|-----|------|------------------|
| 1-3 | torch.compile integration | -20% synthesis time (CUDA), -10% (Metal) |
| 4-5 | Manual kernel fusion (hot paths) | -10% additional reduction |
| 6-7 | Concurrent user testing (10 streams) | Validate scalability claims |
| 8 | CI performance benchmarks | Prevent future regressions |

**Cumulative Gain**: 50-70% total latency reduction vs Phase 1 baseline.

---

### Phase 3: Advanced (Future) - Priority: P2-P3

| Weeks | Task | Expected Outcome |
|-------|------|------------------|
| 1 | CUDA streams (async synthesis) | -15% delivery time (CUDA-only) |
| 2 | CUDA graphs (if applicable) | -30% for repetitive patterns |
| 3+ | Custom kernels (research) | -50%+ for specific ops |

**Cumulative Gain**: Additional 20-30% for advanced CUDA optimizations.

---

## Success Criteria by Phase

### Phase 1 Targets

| Metric | Baseline (est.) | Phase 1 Target | Measurement |
|--------|-----------------|----------------|-------------|
| Synthesis Latency (avg) | 150ms (CPU) / 80ms (GPU) | <50ms | torch.profiler |
| Latency Variance (p95) | Unknown | <80ms | Percentile tracking |
| Audio Glitches | Occasional GC pauses | Zero in 30min test | Manual testing |
| Memory Growth (8hr) | Unknown | <10 MB | tracemalloc |
| GPU Utilization | Unknown | >60% during synthesis | nvidia-smi / Metal HUD |

---

### Phase 2 Targets

| Metric | Phase 1 | Phase 2 Target | Measurement |
|--------|---------|----------------|-------------|
| Synthesis Latency (avg) | <50ms | <30ms | torch.profiler |
| Concurrent Users | 1-2 | 10 with <5% degradation | Load testing |
| Resource Usage | Baseline | -30% CPU vs original | System monitoring |
| Compilation Warmup | N/A | <500ms one-time | Startup profiling |

---

### Phase 3 Targets (CUDA-Specific)

| Metric | Phase 2 | Phase 3 Target | Measurement |
|--------|---------|----------------|-------------|
| Async Overlap | Sequential | 15-25% delivery reduction | NSight timeline |
| Graph Acceleration | N/A | 30-50% (if applicable) | CUDA graph benchmarks |

---

## Risk Assessment Matrix

| Optimization | Quality Risk | Stability Risk | Maintenance Risk | Mitigation |
|--------------|--------------|----------------|------------------|------------|
| Memory Pre-Allocation | üü¢ None | üü¢ Low | üü¢ Low | Deterministic buffers |
| Batch Rendering | üü¢ None | üü° Medium | üü° Medium | Extensive testing |
| torch.no_grad() | üü¢ None | üü¢ None | üü¢ None | Standard practice |
| torch.compile | üü¢ None | üü° Medium | üü° Medium | Warmup testing |
| CUDA Streams | üü¢ None | üü° Medium | üî¥ High | CUDA-only, async complexity |
| Mixed Precision | üî¥ **Critical** | üü¢ Low | üü° Medium | **DO NOT USE** |
| CUDA Graphs | üü¢ None | üî¥ High | üî¥ High | Requires static shapes |
| Custom Kernels | üü° Medium | üî¥ High | üî¥ Very High | Expert review required |

**Legend**:
- üü¢ Low: Minimal risk, standard practice
- üü° Medium: Requires testing, reversible
- üî¥ High: Significant complexity or quality concerns

---

## Final Recommendations

### Immediate Actions (This Week)

1. ‚úÖ **Memory Pre-Allocation** - Implement first (glitch prevention)
2. ‚úÖ **torch.no_grad()** - Trivial, zero risk
3. ‚úÖ **Profiling Baseline** - Foundation for all optimizations

### High-Priority (Next 2 Weeks)

4. ‚úÖ **Batch Chord Rendering** - Highest performance gain
5. ‚ö†Ô∏è **torch.compile** - Test warmup carefully
6. ‚úÖ **Concurrent User Testing** - Validate scalability

### Future Research

7. ‚ö†Ô∏è **CUDA Streams** - CUDA-specific, async complexity
8. ‚ö†Ô∏è **CUDA Graphs** - High effort, static pattern requirement
9. ‚ùå **Mixed Precision** - **REJECTED** for audio quality

### Never Implement

- ‚ùå **FP16/BF16 for Audio** - Unacceptable quality degradation
- ‚ùå **Blocking I/O in Audio Pipeline** - Violates real-time constraint

---

## Monitoring and Validation

### Continuous Metrics (CI Pipeline)

```python
# Automated performance regression tests
@pytest.mark.benchmark
def test_synthesis_latency_regression():
    engine = SynthesisEngine()
    latency_ms = benchmark_render_phrase(engine, iterations=100)

    assert latency_ms < 50, f"Latency {latency_ms:.2f}ms exceeds 50ms target"

@pytest.mark.benchmark
def test_memory_stability():
    monitor = MemoryMonitor()
    for i in range(1000):
        _ = engine.render_phrase(...)

    assert monitor.growth_mb < 10, "Memory leak detected"
```

### Manual Quality Validation

- **A/B Testing**: Compare optimized vs. baseline audio (spectral analysis)
- **Long-Duration Testing**: 8-hour continuous playback (glitch detection)
- **Concurrent Load Testing**: 10 simultaneous WebSocket connections
- **Platform Parity**: Metal vs. CUDA performance within 20%

---

## References

- **Detailed Research**: [research.md](research.md)
- **Quick Reference**: [GPU_OPTIMIZATION_QUICK_REFERENCE.md](GPU_OPTIMIZATION_QUICK_REFERENCE.md)
- **Feature Specification**: [spec.md](spec.md)
- **Implementation Plan**: [plan.md](plan.md)

**Last Updated**: 2025-12-26
